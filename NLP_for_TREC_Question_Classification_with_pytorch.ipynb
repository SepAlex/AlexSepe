{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_for_TREC_Question Classification_with pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing Using Convolutional Nueral Network for Text Retrieval Conference (TREC) Question Classification Dataset with Pytorch**\n",
        "\n",
        "\n",
        "## **Alexander Sepenu**"
      ],
      "metadata": {
        "id": "tHzfqwSWH-1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **This code is made available to beginners in Natural Language Processing to support your learning effort. You can adopt this reproducible code for your use as i hope you find it  purposeful as you continue on your Data Science or Machine Learning journey.** "
      ],
      "metadata": {
        "id": "3QQ8BP86FNQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install old version of pytorch's torchtext  version 0.9.0 to get dataset \n",
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "\n",
        "# Reload environment\n",
        "exit()"
      ],
      "metadata": {
        "id": "Ks46S3a5XUf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbAq1tpSrnar"
      },
      "outputs": [],
      "source": [
        "# Required Libraries \n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Dataset**"
      ],
      "metadata": {
        "id": "l3Dg8CcsuVQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining fields for dataset classification\n",
        "TEXT = data.Field (tokenize = 'spacy', lower = True)\n",
        "LABEL = data.LabelField()"
      ],
      "metadata": {
        "id": "UsG8s0uucfic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting Seed and which Colab RAM size to use from the CUDA package\n",
        "# change colab runtime type to GPU before running code\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "colab_ram = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(colab_ram )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYIoDN_mEMeC",
        "outputId": "5aa7a4ac-595e-456f-f9f2-df7755626f75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into train, test and val sets\n",
        "train, test = datasets.TREC.splits(TEXT, LABEL)\n",
        "train, val = train.split(random_state=random.seed(seed))"
      ],
      "metadata": {
        "id": "RT7mHjSHcwPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the data from train set for label and text structure\n",
        "vars(train[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AgXNaq9dE90",
        "outputId": "9df617ea-54e1-4295-df09-0825a1ccd9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'DESC', 'text': ['what', 'is', 'a', 'cartesian', 'diver', '?']}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary for text and label for words that appearing atleast twice in train set\n",
        "TEXT.build_vocab(train, min_freq = 2)\n",
        "LABEL.build_vocab(train)"
      ],
      "metadata": {
        "id": "FaRiLyMUfCmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the size of the text and label vocabs \n",
        "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
        "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bvypbvUgNlP",
        "outputId": "56e73e21-837a-4a02-9a51-88a726c294e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size of TEXT: 2700\n",
            "Vocabulary size of LABEL: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the iterators for train, test and val sets with batch size = 64 and set to use GPU rantime resource\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, val, test),\n",
        "    batch_size = 64,\n",
        "    sort_key=lambda x: len(x.text), \n",
        "    device = colab_ram)"
      ],
      "metadata": {
        "id": "6O5plub5l7ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Nueral Network Model Building**"
      ],
      "metadata": {
        "id": "SXzBwJa5ujc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining the Convolutional Network Model Params or Layers  \n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocabulary_size, embedding_size, \n",
        "               kernels_number, kernel_sizes, output_size, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
        "    self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels=1, \n",
        "                                                       out_channels= kernels_number,kernel_size= (k, embedding_size))\n",
        "                                                       for k in kernel_sizes])\n",
        "    self.dropout = nn.Dropout(dropout_rate)             # Dropout layer here is to prevent overfitting\n",
        "    self.fully_connected = nn.Linear(len(kernel_sizes) * kernels_number, output_size)\n",
        "\n",
        "## using the forward function to build the model's architecture using defined layers and outputing the prediction of the text\n",
        "  def forward(self, text):\n",
        "    text = text.permute(1,0)\n",
        "    input_embeddings = self.embedding(text)                # Input layer here takes in questions and outputs the corresponding labels\n",
        "    input_embeddings = input_embeddings.unsqueeze(1)\n",
        "    conved = [F.relu(convolution_layer(input_embeddings)).squeeze(3)\n",
        "              for convolution_layer in self.convolution_layers]\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    concat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    final_output = self.fully_connected(concat)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "mDoCrHvPrxKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Convolution Nueral Newtork Params for Modeling building\n",
        "\n",
        "vocabulary_size = 2700              # Input layer size\n",
        "embedding_size = 100                # Dimension of embedding layer\n",
        "kernels_number = 100                # number of filters in network\n",
        "kernel_sizes = [2, 3, 4]            # Filter Sizes for layers\n",
        "output_size = 6                     # len of Label size\n",
        "dropout_rate = 0.8                  # Dropout rate"
      ],
      "metadata": {
        "id": "-8b0FH6Yr1HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing Params in to CNN Model\n",
        "cnn_model = CNN(vocabulary_size, embedding_size, kernels_number, kernel_sizes, output_size, dropout_rate)"
      ],
      "metadata": {
        "id": "PdpHub1rr3Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Running the model in the GPU RAM resource\n",
        "cnn_model.to(colab_ram)"
      ],
      "metadata": {
        "id": "WDI9_CPfsGiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af5c2e0-1087-43ec-f21a-5bb918d45dbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embedding): Embedding(2700, 100)\n",
              "  (convolution_layers): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.8, inplace=False)\n",
              "  (fully_connected): Linear(in_features=300, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "xGCL5QIyv-fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the Model's evaluation criterion\n",
        "eval_criterion = nn.CrossEntropyLoss()         # using the CrossEntropy loss function\n",
        "eval_criterion = eval_criterion.to(colab_ram)   # assigning GPU resoruces to evaluation criterion \n",
        "\n",
        "eval_optimizer = optim.Adam(cnn_model.parameters())  # Model Optimizer using the Adam option"
      ],
      "metadata": {
        "id": "9dagvygRvp4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameters for accuracy in model predition\n",
        "def accuracy(predictions, actual_label):\n",
        "    max_predictions = predictions.argmax(dim= 1, keepdim = True, )\n",
        "    correct_predictions = max_predictions.squeeze(1).eq(actual_label)\n",
        "    accuracy = correct_predictions.sum() / torch.cuda.FloatTensor([actual_label.shape[0]])\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xxthLYkWw97F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definining the parameters to iterate the train set  in batches\n",
        "def train(cnn_model, iterator, eval_optimizer, eval_criterion):\n",
        "\n",
        "    cnn_model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    for batch in iterator:\n",
        "        eval_optimizer.zero_grad()\n",
        "        \n",
        "        predictions = cnn_model(batch.text)\n",
        "        \n",
        "        loss = eval_criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        eval_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "QmWd6iR1zr9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the parameters for the categorical accuracy to calculate the difference between predited and actual text lable\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "metadata": {
        "id": "32osdD-h8tIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Evaluation function parameters to make prediction and to measure loss and accuracy\n",
        "def evaluate(cnn_model, iterator, eval_criterion):\n",
        "\n",
        "    cnn_model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = cnn_model(batch.text)\n",
        "            \n",
        "            loss = eval_criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "           \n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "kyy8C5gv02RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainning the CNN Model**"
      ],
      "metadata": {
        "id": "DrR92MgZ2yeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_epochs = 10\n",
        "\n",
        "best_acc = float('-inf')\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    \n",
        "    # Write the code here\n",
        "    train_loss, train_acc = train(cnn_model, train_iterator, eval_optimizer, eval_criterion)\n",
        "    # Write the code here\n",
        "    valid_loss, valid_acc = evaluate(cnn_model, valid_iterator, eval_criterion)\n",
        "    \n",
        "    if valid_acc > best_acc:\n",
        "        # Write the code here\n",
        "        best_acc = valid_acc\n",
        "        torch.save(cnn_model.state_dict(), 'trec.pt')\n",
        "    \n",
        "    print(f'Epoch {epoch+1} ')\n",
        "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.4f}%')\n",
        "    print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation Acc: {valid_acc*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rglhu6YU2xrM",
        "outputId": "56bb804d-3668-47a0-e3fd-7c33beb7f523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "\tTrain Loss: 0.1763 | Train Acc: 94.5052%\n",
            "\t Validation Loss: 0.535 |  Validation Acc: 83.0395%\n",
            "Epoch 2 \n",
            "\tTrain Loss: 0.1827 | Train Acc: 94.4271%\n",
            "\t Validation Loss: 0.547 |  Validation Acc: 82.9661%\n",
            "Epoch 3 \n",
            "\tTrain Loss: 0.1714 | Train Acc: 94.5521%\n",
            "\t Validation Loss: 0.543 |  Validation Acc: 83.1597%\n",
            "Epoch 4 \n",
            "\tTrain Loss: 0.1667 | Train Acc: 94.4948%\n",
            "\t Validation Loss: 0.549 |  Validation Acc: 82.4519%\n",
            "Epoch 5 \n",
            "\tTrain Loss: 0.1484 | Train Acc: 95.0729%\n",
            "\t Validation Loss: 0.560 |  Validation Acc: 82.9060%\n",
            "Epoch 6 \n",
            "\tTrain Loss: 0.1532 | Train Acc: 94.9740%\n",
            "\t Validation Loss: 0.558 |  Validation Acc: 83.2799%\n",
            "Epoch 7 \n",
            "\tTrain Loss: 0.1436 | Train Acc: 95.0260%\n",
            "\t Validation Loss: 0.560 |  Validation Acc: 83.6872%\n",
            "Epoch 8 \n",
            "\tTrain Loss: 0.1470 | Train Acc: 95.6510%\n",
            "\t Validation Loss: 0.558 |  Validation Acc: 84.0478%\n",
            "Epoch 9 \n",
            "\tTrain Loss: 0.1214 | Train Acc: 96.2708%\n",
            "\t Validation Loss: 0.571 |  Validation Acc: 83.5670%\n",
            "Epoch 10 \n",
            "\tTrain Loss: 0.1184 | Train Acc: 96.1979%\n",
            "\t Validation Loss: 0.562 |  Validation Acc: 83.6138%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to Overfitting on the trainning set and val set, lets test model performance on test set to measure accuracy of the model**"
      ],
      "metadata": {
        "id": "UKt_ed8W-D7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.load_state_dict(torch.load('trec.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(cnn_model, test_iterator, eval_criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9jCO_ia-Dig",
        "outputId": "85322134-39ba-4b74-fec7-3f1c65fba3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.372 | Test Acc: 88.8371%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In conclusion, as the models's parameters were varied, different accuracies were realized. One notable one to share was setting the **dropout_rate = 0.8** and the **number_of_epochs = 10**, the accuracy jumped to **88.8371%**.\n",
        "Due to computational limitations, a grid search was nod concidered to perform and parameter tuning that finds the best model results. I hope as more resoruces become avalable, this and other methods will be explored."
      ],
      "metadata": {
        "id": "w2TnIckTFLDF"
      }
    }
  ]
}