{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_for_TREC_Question Classification_with pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Natural Language Processing Using Convolutional Nueral Network for Text Retrieval Conference (TREC) Question Classification Dataset with Pytorch**\n",
        "\n",
        "\n",
        "## **Alexander Sepenu**"
      ],
      "metadata": {
        "id": "tHzfqwSWH-1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **This code is made available to beginners in Natural Language Processing to support your learning effort. You can adopt this reproducible code for your use as i hope you find it  purposeful as you continue on your Data Science or Machine Learning journey.** "
      ],
      "metadata": {
        "id": "3QQ8BP86FNQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install old version of pytorch's torchtext  version 0.9.0 to get dataset \n",
        "!pip install -U torch==1.8.0 torchtext==0.9.0\n",
        "\n",
        "# Reload environment\n",
        "exit()"
      ],
      "metadata": {
        "id": "Ks46S3a5XUf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cbAq1tpSrnar"
      },
      "outputs": [],
      "source": [
        "# Required Libraries \n",
        "import torch\n",
        "from torchtext.legacy import data, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing Dataset**"
      ],
      "metadata": {
        "id": "l3Dg8CcsuVQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining fields for dataset classification\n",
        "TEXT = data.Field (tokenize = 'spacy', lower = True)\n",
        "LABEL = data.LabelField()"
      ],
      "metadata": {
        "id": "UsG8s0uucfic"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting Seed and which Colab RAM size to use from the CUDA package\n",
        "# change colab runtime type to GPU before running code\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "colab_ram = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(colab_ram )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYIoDN_mEMeC",
        "outputId": "662efe37-5ecd-43a6-e14c-f250055089b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data into train, test and val sets\n",
        "train, test = datasets.TREC.splits(TEXT, LABEL)\n",
        "train, val = train.split(random_state=random.seed(seed))"
      ],
      "metadata": {
        "id": "RT7mHjSHcwPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1777f9c1-8c2b-493c-c775-530dc53ccf01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading train_5500.label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "train_5500.label: 100%|██████████| 336k/336k [00:00<00:00, 3.01MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading TREC_10.label\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TREC_10.label: 100%|██████████| 23.4k/23.4k [00:00<00:00, 889kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the data from train set for label and text structure\n",
        "vars(train[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AgXNaq9dE90",
        "outputId": "5ef3550f-f84c-4465-bcfd-2610d381eb60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 'DESC', 'text': ['what', 'is', 'a', 'cartesian', 'diver', '?']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build vocabulary for text and label for words that appearing atleast twice in train set\n",
        "TEXT.build_vocab(train, min_freq = 2)\n",
        "LABEL.build_vocab(train)"
      ],
      "metadata": {
        "id": "FaRiLyMUfCmk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the size of the text and label vocabs \n",
        "print(\"Vocabulary size of TEXT:\",len(TEXT.vocab.stoi))\n",
        "print(\"Vocabulary size of LABEL:\",len(LABEL.vocab.stoi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bvypbvUgNlP",
        "outputId": "a39ba9f3-97fc-41e2-8988-7ca4a212feb9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size of TEXT: 2700\n",
            "Vocabulary size of LABEL: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the iterators for train, test and val sets with batch size = 64 and set to use GPU rantime resource\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((train, val, test),\n",
        "    batch_size = 64,\n",
        "    sort_key=lambda x: len(x.text), \n",
        "    device = colab_ram)"
      ],
      "metadata": {
        "id": "6O5plub5l7ut"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convolutional Nueral Network Model Building**"
      ],
      "metadata": {
        "id": "SXzBwJa5ujc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining the Convolutional Network Model Params or Layers  \n",
        "class CNN(nn.Module):\n",
        "  def __init__(self, vocabulary_size, embedding_size, \n",
        "               kernels_number, kernel_sizes, output_size, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
        "    self.convolution_layers = nn.ModuleList([nn.Conv2d(in_channels=1, \n",
        "                                                       out_channels= kernels_number,kernel_size= (k, embedding_size))\n",
        "                                                       for k in kernel_sizes])\n",
        "    self.dropout = nn.Dropout(dropout_rate)             # Dropout layer here is to prevent overfitting\n",
        "    self.fully_connected = nn.Linear(len(kernel_sizes) * kernels_number, output_size)\n",
        "\n",
        "## using the forward function to build the model's architecture using defined layers and outputing the prediction of the text\n",
        "  def forward(self, text):\n",
        "    text = text.permute(1,0)\n",
        "    input_embeddings = self.embedding(text)                # Input layer here takes in questions and outputs the corresponding labels\n",
        "    input_embeddings = input_embeddings.unsqueeze(1)\n",
        "    conved = [F.relu(convolution_layer(input_embeddings)).squeeze(3)\n",
        "              for convolution_layer in self.convolution_layers]\n",
        "    pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "    concat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "    final_output = self.fully_connected(concat)\n",
        "\n",
        "    return final_output"
      ],
      "metadata": {
        "id": "mDoCrHvPrxKk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining Convolution Nueral Newtork Params for Modeling building\n",
        "\n",
        "vocabulary_size = 2700              # Input layer size\n",
        "embedding_size = 100                # Dimension of embedding layer\n",
        "kernels_number = 100                # number of filters in network\n",
        "kernel_sizes = [2, 3, 4]            # Filter Sizes for layers\n",
        "output_size = 6                     # len of Label size\n",
        "dropout_rate = 0.8                  # Dropout rate"
      ],
      "metadata": {
        "id": "-8b0FH6Yr1HA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passing Params in to CNN Model\n",
        "cnn_model = CNN(vocabulary_size, embedding_size, kernels_number, kernel_sizes, output_size, dropout_rate)"
      ],
      "metadata": {
        "id": "PdpHub1rr3Pp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Running the model in the GPU RAM resource\n",
        "cnn_model.to(colab_ram)"
      ],
      "metadata": {
        "id": "WDI9_CPfsGiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7c8a68-e9c5-411d-d721-92ac5d5d97e3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (embedding): Embedding(2700, 100)\n",
              "  (convolution_layers): ModuleList(\n",
              "    (0): Conv2d(1, 100, kernel_size=(2, 100), stride=(1, 1))\n",
              "    (1): Conv2d(1, 100, kernel_size=(3, 100), stride=(1, 1))\n",
              "    (2): Conv2d(1, 100, kernel_size=(4, 100), stride=(1, 1))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.8, inplace=False)\n",
              "  (fully_connected): Linear(in_features=300, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "xGCL5QIyv-fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the Model's evaluation criterion\n",
        "eval_criterion = nn.CrossEntropyLoss()         # using the CrossEntropy loss function\n",
        "eval_criterion = eval_criterion.to(colab_ram)   # assigning GPU resoruces to evaluation criterion \n",
        "\n",
        "eval_optimizer = optim.Adam(cnn_model.parameters())  # Model Optimizer using the Adam option"
      ],
      "metadata": {
        "id": "9dagvygRvp4-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining parameters for accuracy in model predition\n",
        "def accuracy(predictions, actual_label):\n",
        "    max_predictions = predictions.argmax(dim= 1, keepdim = True, )\n",
        "    correct_predictions = max_predictions.squeeze(1).eq(actual_label)\n",
        "    accuracy = correct_predictions.sum() / torch.cuda.FloatTensor([actual_label.shape[0]])\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "xxthLYkWw97F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definining the parameters to iterate the train set  in batches\n",
        "def train(cnn_model, iterator, eval_optimizer, eval_criterion):\n",
        "\n",
        "    cnn_model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    for batch in iterator:\n",
        "        eval_optimizer.zero_grad()\n",
        "        \n",
        "        predictions = cnn_model(batch.text)\n",
        "        \n",
        "        loss = eval_criterion(predictions, batch.label)\n",
        "        \n",
        "        acc = accuracy(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        eval_optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "QmWd6iR1zr9h"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the parameters for the categorical accuracy to calculate the difference between predited and actual text lable\n",
        "def categorical_accuracy(preds, y):\n",
        "    top_pred = preds.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ],
      "metadata": {
        "id": "32osdD-h8tIo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Evaluation function parameters to make prediction and to measure loss and accuracy\n",
        "def evaluate(cnn_model, iterator, eval_criterion):\n",
        "\n",
        "    cnn_model.eval()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "\n",
        "            predictions = cnn_model(batch.text)\n",
        "            \n",
        "            loss = eval_criterion(predictions, batch.label)\n",
        "            \n",
        "            acc = categorical_accuracy(predictions, batch.label)\n",
        "           \n",
        "            epoch_loss += loss.item()\n",
        "            \n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "metadata": {
        "id": "kyy8C5gv02RL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trainning the CNN Model**"
      ],
      "metadata": {
        "id": "DrR92MgZ2yeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_epochs = 10\n",
        "\n",
        "best_acc = float('-inf')\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "    \n",
        "    train_loss, train_acc = train(cnn_model, train_iterator, eval_optimizer, eval_criterion)\n",
        "    valid_loss, valid_acc = evaluate(cnn_model, valid_iterator, eval_criterion)\n",
        "    \n",
        "    if valid_acc > best_acc:\n",
        "        best_acc = valid_acc\n",
        "        torch.save(cnn_model.state_dict(), 'trec.pt')\n",
        "    \n",
        "    print(f'Epoch {epoch+1} ')\n",
        "    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.4f}%')\n",
        "    print(f'\\t Validation Loss: {valid_loss:.3f} |  Validation Acc: {valid_acc*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rglhu6YU2xrM",
        "outputId": "0f60d6ee-4db0-459d-a13a-12025085c858"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            "\tTrain Loss: 1.7619 | Train Acc: 30.5833%\n",
            "\t Validation Loss: 1.177 |  Validation Acc: 58.5136%\n",
            "Epoch 2 \n",
            "\tTrain Loss: 1.2546 | Train Acc: 51.7396%\n",
            "\t Validation Loss: 0.964 |  Validation Acc: 66.3929%\n",
            "Epoch 3 \n",
            "\tTrain Loss: 1.0641 | Train Acc: 59.3073%\n",
            "\t Validation Loss: 0.861 |  Validation Acc: 69.0104%\n",
            "Epoch 4 \n",
            "\tTrain Loss: 0.9461 | Train Acc: 63.5417%\n",
            "\t Validation Loss: 0.786 |  Validation Acc: 71.4476%\n",
            "Epoch 5 \n",
            "\tTrain Loss: 0.8685 | Train Acc: 67.0885%\n",
            "\t Validation Loss: 0.747 |  Validation Acc: 72.6629%\n",
            "Epoch 6 \n",
            "\tTrain Loss: 0.8036 | Train Acc: 70.5573%\n",
            "\t Validation Loss: 0.721 |  Validation Acc: 73.9450%\n",
            "Epoch 7 \n",
            "\tTrain Loss: 0.7404 | Train Acc: 72.7135%\n",
            "\t Validation Loss: 0.697 |  Validation Acc: 74.9332%\n",
            "Epoch 8 \n",
            "\tTrain Loss: 0.6624 | Train Acc: 76.2708%\n",
            "\t Validation Loss: 0.651 |  Validation Acc: 77.5240%\n",
            "Epoch 9 \n",
            "\tTrain Loss: 0.6346 | Train Acc: 77.4792%\n",
            "\t Validation Loss: 0.633 |  Validation Acc: 77.2970%\n",
            "Epoch 10 \n",
            "\tTrain Loss: 0.6002 | Train Acc: 78.3854%\n",
            "\t Validation Loss: 0.623 |  Validation Acc: 78.1384%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Due to Overfitting on the trainning set and val set, lets test model performance on test set to measure accuracy of the model**"
      ],
      "metadata": {
        "id": "UKt_ed8W-D7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.load_state_dict(torch.load('trec.pt'))\n",
        "\n",
        "test_loss, test_acc = evaluate(cnn_model, test_iterator, eval_criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.4f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9jCO_ia-Dig",
        "outputId": "fe7fb585-e7e1-43ec-92ef-a508086e4343"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.543 | Test Acc: 82.8425%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def predict_class(cnn_model, sentence, min_len = 4):\n",
        "    cnn_model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    if len(tokenized) < min_len:\n",
        "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(colab_ram)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    preds = cnn_model(tensor)\n",
        "    max_preds = preds.argmax(dim = 1)\n",
        "    return max_preds.item()"
      ],
      "metadata": {
        "id": "kMcjtiDLJ95L"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing how well the model predicts a label with a text input**"
      ],
      "metadata": {
        "id": "Hym8dYNXLdEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class = predict_class(cnn_model, \"How many zeros are in a thousand\")\n",
        "print(f'Predicted Lable is: {LABEL.vocab.itos[pred_class]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pr2CUaXxKFC7",
        "outputId": "3bc740f5-94cd-4a09-bcdc-18c9f4805f45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Lable is: NUM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class = predict_class(cnn_model, \"Everest is the tallest mountain in the world\")\n",
        "print(f'Predicted Lable is: {LABEL.vocab.itos[pred_class]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTOvgr46MNao",
        "outputId": "551b42c8-1224-4dbc-cfe5-7097ad84af3d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Lable is: LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_class = predict_class(cnn_model, \"Where is my Burger?\")\n",
        "print(f'Predicted Lable is: {LABEL.vocab.itos[pred_class]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HldlOLR_NFMk",
        "outputId": "5a848ac0-ac53-4716-c1dd-e741ccdcce9d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Lable is: DESC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "In conclusion, as the models's parameters were varied, different accuracies were realized. One notable one to share was setting the **dropout_rate = 0.8** and the **number_of_epochs = 10**, the accuracy jumped to **88.8371%**. Perhaps as more data becomes availbale varied parameters could yield better results.\n",
        "Due to computational limitations, a grid search was nod concidered to perform and parameter tuning that finds the best model results. I hope as more resources become available, this and other methods will be explored."
      ],
      "metadata": {
        "id": "w2TnIckTFLDF"
      }
    }
  ]
}