{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8f42ae",
   "metadata": {},
   "source": [
    "# <h1><center> Quantifying the World </center></h1>\n",
    "##  <h2><center> Case Study 1 </center></h2> \n",
    "### <h3><center> Presented by: </center></h3> \n",
    "### Alexander Sepenu\n",
    "### Nnenna Okpara\n",
    "### Taifur Chowdhury\n",
    "### Edgar Nunez - Gonzalez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0e31d",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a54f3d",
   "metadata": {},
   "source": [
    "Objective: The objective of this case study is to identify the most influential variables on our linear regression model designed to predict the critical temperatures at which different material compositions become superconductors.\n",
    "\n",
    "Problem Statement: We have two datasets regarding a study of the critical temperatures of superconductors. The first file contains all the variations of the measures for critical temperatures of superconductors. The second file contains the chemical composition associated with the critical temperature of the superconductors. Given this information, design a model to help us determine whether the critical temperature of a material composition yields a superconductor and the critical temperature at which the material compositions in the study become superconductors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936a0c7",
   "metadata": {},
   "source": [
    "# Data Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c6dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16aa992",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ed705",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading CSV files \n",
    "train = pd.read_csv('train.csv')\n",
    "unique = pd.read_csv('unique_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the file read\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of the file read\n",
    "unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd13399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing last columns - \"critical temperature\" and combining both tables to create a dataframe \n",
    "train_2 = train.iloc[:, :-1]\n",
    "unique_2 = unique.iloc[:, :-1]\n",
    "df_main = pd.concat([train_2, unique_2], axis=1, join=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724fdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe dimension should equal (88+81) 169 columns and 21,263 rows\n",
    "df_main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5039fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the statistical features of the new dataframe\n",
    "df_main.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d595ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove outliers\n",
    "#df_main = df_main[(df_main.critical_temp >0.0125) & (df_main.critical_temp <133.6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36808c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf69ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all cloumns to float to normalize data for ease of use\n",
    "df_main = df_main[:].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eaf10f",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_main.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19911f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af05799",
   "metadata": {},
   "source": [
    "# EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c19790",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a90cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_main, title=\"Profiling Report\", explorative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0340659",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this line of code does an in depth EDA but take about 4 hours to complete depend on computing power\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code saves the results into your working directory which you can open in your brower to learn more about the data\n",
    "profile.to_file('crit_temp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caabf8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA Package to select features of relevance based on correlation with target variable\n",
    "#opens in a new brower window to save the selected columns that have significance\n",
    "#source: https://www.investopedia.com/terms/c/correlationcoefficient.asp#:~:text=Values%20at%20or%20close%20to,0.8%20are%20not%20considered%20significant.\n",
    "import pandas as pd\n",
    "import dtale\n",
    "dtale.show(df_main, open_browser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc81a6",
   "metadata": {},
   "source": [
    "### EDA Output from Dtale Package\n",
    "![alt text](dtale_reults.jpg \"EDA Output from Dtale Package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde5c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the report\n",
    "import sweetviz as sv\n",
    "\n",
    "temp_report = sv.analyze(df_main)\n",
    "\n",
    "#display the report\n",
    "temp_report.show_html('temp.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b2ace",
   "metadata": {},
   "source": [
    "### EDA Output from Sweetviz Package\n",
    "Below is the EDA report of the semi conductor dataset to learn the distribution of the various features. \n",
    "![alt text](EDA_1.jpg \"EDA Output from Dtale Package\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fdfe6a",
   "metadata": {},
   "source": [
    "#### EDA Output from Sweetviz Package Cont'd\n",
    "![alt text](EDA_3.jpg \"EDA Output from Sweetviz Package Cont'd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f48d93",
   "metadata": {},
   "source": [
    "#### EDA Output from Sweetviz Package Cont'd\n",
    "![alt text](EDA_4.jpg \"EDA Output from Sweetviz Package Cont'd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263109e",
   "metadata": {},
   "source": [
    "Since we have been provided with two datasets, we combined both dataset so then we can preprocess our main dataframe for the model. We have 168 columns and 21,263 rows for our mainframe. We do not have any missing values for our dataframe. For our exploratory data analysis, we have used the packages called ‘profile’, 'sweetviz' and 'dtale'. It was able to tell us how our data was distributed. From the EDA the dataset was heavily unbalanced hence would require some transformations for model building. The package has provided us the data distribution report for all the features.\n",
    "\n",
    "Dtale package was use to select features of relevance based on their correlation with Critical_temp(target) between 1 and 0.8. This removed the cloumn that are highly colinear with out target variable. We have gathered a list of important features in a document which lists the important features for our model building. 59 features out of 168 were selected and are listed in the dtale report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a7715a",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010c63b8",
   "metadata": {},
   "source": [
    "To solve the problem, we have used linear regression to predict the critical temperature using those important features that are described above. This method will have use to tackle our objective of predicting the temperature values and finding out how the important features have contributed to our prediction. The following metrics were used to evaluate our models (L1 and L2) of which the best model was chosen.\n",
    "\n",
    "### Evaluation Metrics: \n",
    "\n",
    "Root Mean Squared Error (RMSE): measures the standard deviation between residuals which are the variance between actual values and predicted values. It measures the distance of how far the actual values are from the values in regression line. The formula first divides the variance squared by number of values in the dataset (n) and then takes the squared root of the value. It is widely used for linear forecasting and regression; we will use this as primary metrics for model evaluation.\n",
    "\n",
    "#### Source : https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e\n",
    "\n",
    "R-Squared: It is a statistical measurement of the proportion of the variance for the dependent variable that is explained by an independent variable. In other words, it tells us how well the dependent variable has been explained by the independent variables. The measurement is varied from 0% to 100% - 0% being the model explains none of the variability of the response variable around the mean, and 100% being the model explains all the variability of the response variable around the mean. It has some limitations it cannot determine whether the coefficient estimates and predictions are biased. The Higher the R-squared, the better the model. \n",
    "\n",
    "#### Source: https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit \n",
    "\n",
    "\n",
    "MAE measures the prediction error. It would be used to calculate the absolute difference between the actual and predicted values. MAE does not penalize large errors. \n",
    "MAE is less sensitive to outliers compared to RMSE. The MAE can be calculated as follows: \n",
    "•\tMAE = 1 / N * sum for i to N abs(y_i – yhat_i) \n",
    "Where y_i is the i’th expected value in the dataset, yhat_i is the i’th predicted value and abs()is the absolute function. \n",
    "\n",
    "\n",
    "MSE is the average squared difference between the observed actual outcome values and the values predicted by the model.  The MSE is calculated as the mean or average of the squared differences between predicted and expected target values in a dataset. \n",
    "•\tMSE = 1 / N * sum for i to N (y_i – yhat_i)^2 \n",
    "Where y_i is the i’th expected value in the dataset and yhat_i is the i’th predicted value. The difference between these two values is squared, which has the effect of removing the sign, resulting in a positive error value. \n",
    "\n",
    "\n",
    "AIC works by evaluating the model’s fit on the training data and adding a penalty term for the complexity of the model. The desired result is to find the lowest possible AIC, which indicates the best balance of model fit with generalizability. This serves the eventual goal of maximizing fit on out-of-sample data. The lower the AIC, the better the model. \n",
    " \n",
    "#### Sources:\n",
    " http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/ \n",
    " \n",
    "https://towardsdatascience.com/evaluation-metrics-model-selection-in-linear-regression-73c7573208be \n",
    " \n",
    "https://machinelearningmastery.com/regression-metrics-for-machine-learning/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea0aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "###### Import required Python packages #####\n",
    "############################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "import numpy as np\n",
    "from ipywidgets import interact, interactive, IntSlider, Layout\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from sklearn.model_selection import train_test_split\n",
    "import io\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76acea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a58c22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96706961",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_working_directory = widgets.Text(\n",
    "    value=os.getcwd(),\n",
    "    placeholder='C:/Users/korku/Documents/MY SMU COURSES/Semester 2/Spring 2022/Quantifying the World/Case Study 1',\n",
    "    description='Directory:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='100%')\n",
    ")\n",
    "\n",
    "display(set_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.chdir(set_working_directory.value)\n",
    "    print('Changed directory to {}'.format(set_working_directory.value))\n",
    "except Exception as e:\n",
    "    print('Failed to change directory')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "##### Data Ingestion Functions\n",
    "########################################\n",
    "\n",
    "def compile_raw_data(filename, tab_names, subfolder, delimiter_char = ',', skip_rows = 0, file_ext = 'csv'):\n",
    "    \n",
    "    # Inputs: \n",
    "    ## filename = 'sample.csv' | 'sample.xlsx' - the filename in the directory (including the extension) \n",
    "    ## tab_names = None | ['Sheet1,'Sheet2'] - None for csv; [comma separated list of tab names] for xlsx\n",
    "    ## subfolder = 'source_data' - string containing the name of a folder in the working directory\n",
    "    ## delimiter_char = ',' | ';' - None for xlsx\n",
    "    ## rows to skip = default 0 - Not used for csv; trims the user-defined number of rows from an xlsx\n",
    "    ## file extension = csv | xlsx\n",
    "    \n",
    "    # Description: reads in the workbook; standardizes header names; \n",
    "    # Outputs: returns a dictionary of dataframes\n",
    "\n",
    "    master_data = {}\n",
    "    if subfolder:\n",
    "        file_path = subfolder+'/{}'.format(filename)\n",
    "    else:\n",
    "        file_path = filename\n",
    "\n",
    "    if file_ext == 'csv':\n",
    "        tab_names = [re.sub('.csv','', filename)]\n",
    "\n",
    "    for tab in tab_names:\n",
    "        try:\n",
    "            if file_ext == 'xlsx':\n",
    "                dframe = pd.read_excel(file_path, tab, skip_rows)\n",
    "            elif file_ext == 'csv' and delimiter_char == ',':\n",
    "                dframe = pd.read_csv(file_path, header=0, delimiter=',')\n",
    "            else:\n",
    "                dframe = pd.read_csv(file_path, header=0, delimiter=';')\n",
    "                \n",
    "            sanitizer = {\n",
    "                        '$':'USD',\n",
    "                        '(':' ',\n",
    "                        ')':' ',\n",
    "                        '/':' ',\n",
    "                        '-':' ',\n",
    "                        ',':' ',\n",
    "                        '.':' '\n",
    "            }\n",
    "                        \n",
    "            for key, value in sanitizer.items():\n",
    "                dframe.rename(columns=lambda x: x.replace(key, value), inplace=True)\n",
    "                \n",
    "            dframe.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "            dframe.rename(columns=lambda x: re.sub(' +','_', x), inplace=True)\n",
    "            \n",
    "            dframe.columns = map(str.lower, dframe.columns)\n",
    "            \n",
    "            master_data.update({tab:dframe})\n",
    "        except Exception as e:\n",
    "            master_data.update({tab:'Failed'})\n",
    "    \n",
    "    return master_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6137c446",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_type = widgets.RadioButtons(\n",
    "    options=['local', 'url'],\n",
    "    description='File Location:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "upload_url = widgets.Text(\n",
    "    value='scaled_down_df.csv',\n",
    "    placeholder='http://',\n",
    "    description='URL:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='80%')\n",
    ")\n",
    "upload_filename = widgets.Text(\n",
    "    value='scaled_down_df.csv',\n",
    "    placeholder='Sample File.csv',\n",
    "    description='File Name:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "file_type = widgets.RadioButtons(\n",
    "    options=['csv', 'xlsx'],\n",
    "    description='File Type:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "tab_names = widgets.Text(\n",
    "    value='Sheet1, Sheet2, Sheet3, etc',\n",
    "    placeholder='ALL EMPLOYEES, PAST EMPLOYEES',\n",
    "    description='Tab(s):',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "subfolder_name = widgets.Text(\n",
    "    value='source_data',\n",
    "    placeholder='Subfolder name',\n",
    "    description='Subfolder:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "subfolder = widgets.RadioButtons(\n",
    "    options=['no','yes'],\n",
    "    value='no',\n",
    "    description='Subfolder:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "skip_rows = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Skip Rows:',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "delimiter = widgets.RadioButtons(\n",
    "    options=[',',';'],\n",
    "    value=',',\n",
    "    description='Delimiter:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def text_field(x):\n",
    "    if(x=='xlsx'):\n",
    "        display(tab_names)\n",
    "        tab_names.on_submit(tab_names)\n",
    "        display(skip_rows)\n",
    "    else:\n",
    "        display(delimiter)\n",
    "        print('Tab Names: Not needed for csv files')\n",
    "\n",
    "def sub_folder(y):\n",
    "    if(y=='yes'):\n",
    "        display(subfolder_name)\n",
    "        subfolder_name.on_submit(subfolder_name)\n",
    "    else:\n",
    "        print('Using {} folder'.format(os.getcwd()))\n",
    "\n",
    "def file_location(z):\n",
    "    if(z=='local'):\n",
    "        display(upload_filename)\n",
    "        i = widgets.interactive(text_field, x=file_type)\n",
    "        display(i)\n",
    "        p = widgets.interactive(sub_folder, y=subfolder)\n",
    "        display(p)\n",
    "    else:\n",
    "        display(upload_url)\n",
    "    \n",
    "q = widgets.interactive(file_location, z=upload_type)\n",
    "\n",
    "display(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data = {}\n",
    "\n",
    "if upload_type.value == 'url':\n",
    "    url_response = requests.request(\"GET\", upload_url.value)\n",
    "    master_data['url_data'] = pd.read_csv(io.BytesIO(url_response.content))\n",
    "else:\n",
    "    if file_type.value == 'csv':\n",
    "        tabs = None\n",
    "        skiprows = 0\n",
    "    else:\n",
    "        tabs = [x.strip() for x in tab_names.value.split(',')]\n",
    "        skiprows = skip_rows.value\n",
    "\n",
    "    if subfolder.value == 'yes':\n",
    "        subfolder = subfolder_name.value\n",
    "    else:\n",
    "        subfolder = None\n",
    "    master_data = compile_raw_data(upload_filename.value, tabs, subfolder, delimiter_char = delimiter.value, skip_rows = skiprows, file_ext = file_type.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in master_data.items():\n",
    "    try:\n",
    "        print('{} table was ingested with {} rows and {} columns'.format(key,value.shape[0],value.shape[1]))\n",
    "    except:\n",
    "        print('{} table failed to load'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6df475",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keys = widgets.Select(\n",
    "    options=master_data.keys(),\n",
    "    description='Tables:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "display(dict_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data[dict_keys.value].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_variables = widgets.SelectMultiple(\n",
    "    options=master_data[dict_keys.value].columns.tolist(),\n",
    "    description='Variables:',\n",
    "    disabled=False,\n",
    "    layout=Layout(width='50%')\n",
    ")\n",
    "\n",
    "display(review_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1121c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_var_list = []\n",
    "for i in review_variables.value:\n",
    "    review_var_list.append(i)\n",
    "    \n",
    "master_data['custom_table'] = master_data[dict_keys.value][review_var_list]\n",
    "\n",
    "head_number = widgets.BoundedIntText(\n",
    "    value=5,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    step=1,\n",
    "    description='Rows:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "def sample_view(head_number):\n",
    "    sample = master_data['custom_table'].head(head_number)\n",
    "    print(sample)\n",
    "\n",
    "out = widgets.interactive_output(sample_view, {'head_number':head_number})\n",
    "\n",
    "widgets.VBox([widgets.VBox([head_number]), out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956a780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = widgets.Select(\n",
    "    options=master_data['custom_table'].columns.tolist(),\n",
    "    description='Target',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "target_type = widgets.Select(\n",
    "    options=['Continuous','Categorical'],\n",
    "    description='Type',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(target)\n",
    "display(target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044e286",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    h2o.cluster().shutdown()\n",
    "    h2o.init()\n",
    "except:\n",
    "    h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee55679",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = master_data['custom_table'].dropna(subset=[target.value])\n",
    "model_df = h2o.H2OFrame(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b1aaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_type.value == 'Categorical':\n",
    "    model_df[target.value] = model_df[target.value].asfactor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c5b39f",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dce6dd",
   "metadata": {},
   "source": [
    "Data was loaded into H2o package and a train test split of 70 to 30 was done to model and evaluate the model. The General Linear Model approach was selected to build the linear regression in which the regularaization technique was used to achieve model results. Parameters were defined to proceed with modeling of which alpha was set to 1 for L1 and 0 for L2 whiles lambda was set to  greater than 0 in the range\n",
    "\n",
    "What is Regularization?\n",
    "\n",
    "Simply, it is a technique that prevents the model from overfitting by adding extra information to it. In cases of overfitting, the model may not be able to predict the output when it deals with unseen data and adds noise to the output. Regularization reduces (or regularizes) the magnitude of the variables while at the same time keeps the number of features. In that way, the noise gets reduced. \n",
    "Mathematically, regularization minimizes the cost function. It introduces penalty in our loss function. Regularization penalizes the slope, m, as it increases – both in positive scale and negative scale. In that way, the magnitude of the variables gets reduced. Unless there is a very strong interaction that lowers the loss significantly, we want to keep the absolute value of the slopes small. Lambda is the strength of the penalization. As lambda increases, the parameters become smaller which reduces overfitting.\n",
    "\n",
    "\n",
    "\n",
    "We need to find a function that penalizes weak correlations and promotes strong correlations. The following are the two forms of the functions:\n",
    "\n",
    "L1 Regularization: It is also called Lasso Regression or first order regularization. The function of the penalty is the absolute value of the coefficient. It primarily helps us with variable selection. It introduces sparsity our equation. As we increase the value of lambda, some of the slope to zero and others to non-zero. We end up keeping the non-zero-coefficient variables because those are the important ones. This process overall makes the primary use as feature selection.\n",
    "\n",
    "L2 Regularization: It is also called Ridge regularization or second order regularization. The function of the penalty is the square of the coefficient. It does not induce sparsity. It only allows variable to contribute. It primarily used to prevent overfitting.\n",
    "\n",
    "\n",
    "Source: https://www.javatpoint.com/regularization-in-machine-learning\n",
    "\n",
    "Below is a summaraized output of results from H2O detailing the comparison between L1 and L2 models in which L2(Ridge Regression) was chosen based on the metrics mentioned above because the predicted values were closer to the actual values used in the dataset.MSE, MAE, RMSE and AIC scored the lowest which means the predicted values were closer to actual values from that model. R-sqaure was actually higher in L2 compared to that from L1 due to explanabilty defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa64f4",
   "metadata": {},
   "source": [
    "### Sampling Techniques used was 70 to 30 split in H2O Package\n",
    "![alt text](Train_and_Test_data_Split.jpg \"Sampling Techniques used was 70 to 30 split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3880fe48",
   "metadata": {},
   "source": [
    "   ### Table results of model metrics for comparison to choose prefered model\n",
    "![alt text](Model_Selection_Metrics.jpg \"Table results of model metrics for comparison to choose prefered model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76a86b",
   "metadata": {},
   "source": [
    "# Model Interpretability & Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01293f",
   "metadata": {},
   "source": [
    "Below are the results images from the model build in which the coefficients are generated to help interpret the linear regression model.\n",
    "\n",
    "From image5, the feature importance from the L1 model is shown in which just 14 features were used to interpret the model whereas in image6 the feature importance from the L2 model is shown with 58 variables used to achieve the best model performance.\n",
    "\n",
    "In image6 and image7, the coefficients of entire linear regression model is shown with their signs (positve or negative) to showtheir relationship with the target variable of interest.\n",
    "\n",
    "From these results, the top five is analyzed to interpret the model. \n",
    "\n",
    "critical_temp = -79.3245 - 0.0074(number_of_elements) + 0.0572(mean_atomic_mass) + \t0.0572(mean_atomic_mass) -0.0601(wtd_mean_atomic_mass) + 0.0569(gmean_atomic_mass) + 0.0817(wtd_gmean_atomic_mass) ... N(M)\n",
    "\n",
    "where N is cofficient and M is the variable.\n",
    "\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "#### Intercept\n",
    "From the above, the model can be interpreted as holding all variable at zero, the critical_temp will have a negative effect on the superconductor at its intercept\n",
    "\n",
    "#### All Betas\n",
    "Also, holding the intercept and all other variables constant except for number_of_elements, the critical_temp will have a negative effect on the superconductor etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99268372",
   "metadata": {},
   "source": [
    "### Image of feauture importance in L1 (LASSO)\n",
    "![alt text](Feature_importance_L1.jpg \"Image of feauture importance in L1 (LASSO)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cebb87",
   "metadata": {},
   "source": [
    "### Image of feauture importance in L2 (Ridge)\n",
    "![alt text](Feature_importance_L2.jpg \"Image of feauture importance in L2 (Ridge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44e687",
   "metadata": {},
   "source": [
    "### Image of model coefficients\n",
    "![alt text](Model_Coefficients_1.jpg \"Image of model coefficients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111735f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image of model coefficients\n",
    "from PIL import Image\n",
    "myImage8 = Image.open(\"Model_Coefficients_2.jpg\");\n",
    "myImage8.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45718dc",
   "metadata": {},
   "source": [
    "### Image of model coefficients Cont'd\n",
    "![alt text](Model_Coefficients_2.jpg \"Image of model coefficients Cnot'd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9149d47",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We are proposing to use the Ridge regression (L2) to predict critical temperatures of superconducting materials. This is based on our analysis in comparing the model results of MAE, MSE, RMSE, AIC against different modeling techniques.  The L2 had lower evaluation metrics than the other models.\n",
    "\n",
    "Our prediction model using (L2) helped us predict the critical temperatures at which the majority of the material compositions in the study become superconductors. In addition, we were able to determine which are the set of variables with the most influence on our model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
